{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimento à partir do Twitter em Português\n",
    "\n",
    "Por ser uma rede com restrição a 280 caracteres, os usuários são obrigados a se expressar com objetividade, com menor margem para figuras de linguagem complexas.\n",
    "Foi concebido para o envio de informações efêmeras, no início o objetivo era apenas publicar o que os usuários estavam fazendo naquela hora. \n",
    "\n",
    "Hoje é um verdadeiro \"radar\" dos acontecimentos mundiais. É possível ter uma boa ideia do que se passa no mundo, através do Twitter, o que faz dele uma boa fonte de dados para criação do modelo.\n",
    "\n",
    "Criado o modelo, o objetvo é utilizá-lo para \"medir\" o sentimento de mensagens com determinado conteúdo ou hashtag. Dada quantidade de mensagens publicadas, é impossível lê-las todas para descobrir se o sentimento geral sobre determinado assunto é bom ou ruim. Um modelo pode fazer isso em minutos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos com a biblioteca básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extração dos tuítes, utilizaremos o Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import pandas as pd\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import Stream\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#tratamento das Strings\n",
    "import re  \n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acessar o Twitter e \"puxar\" o conteúdo, você vai precisar se cadastrar  e obter as chaves de acesso da API do Twitter.\n",
    "Você consegue isso criando e cadastrando uma aplicação no próprio twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chaves de acesso\n",
    "\n",
    "consumer_key   = 'nbnvbcnvbncbvncbvncbvncbvn'\n",
    "consumer_secret = 'nbnvbcnvbncbvncbvncbvncbvn'\n",
    "access_token = 'nbnvbcnvbncbvncbvncbvncbvn'\n",
    "access_secret = 'nbnvbcnvbncbvncbvncbvncbvn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captura\n",
    "\n",
    "Função para extrair as mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "def tweetSearch(query, limit = 10000, language = \"pt\"):\n",
    "    message,favorite_count,followers_count, retweet_count,created_at,user_name,retificado=[],[],[],[],[],[],[]\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')  #elimina palavras curtas menos de 2 letras\n",
    "    for tweet in tweepy.Cursor(api.search,q=query , count=limit, result_type=\"recent\", include_entities=True,lang=language, tweet_mode='extended').items(limit):\n",
    "      if (not tweet.retweeted) and ('RT @' not in tweet.full_text): #exclui retuítes\n",
    "        text=tweet.full_text\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"_\", \"\", text)\n",
    "        text = re.sub(r\"@\\S+\", \"\", text)   #tira referências a usuários\n",
    "        text = re.sub(r\"#\\S+\", \"\", text)   #tira hashtags\n",
    "        text = re.sub(r\"\\n\", \"\", text)   #tira new line\n",
    "        message.append(text)\n",
    "        encurtado=shortword.sub('',text)  # tira palavras pequenas\n",
    "        retificado.append(encurtado.lower())\n",
    "        created_at.append(tweet.created_at)\n",
    "        user_name.append(tweet.user.name)\n",
    "        retweet_count.append(tweet.retweet_count)\n",
    "        followers_count.append(tweet.user.followers_count)\n",
    "        df=pd.DataFrame({'Message':message, \n",
    "                'Retificado':retificado,\n",
    "                'Criado':created_at,\n",
    "                'Usuário':user_name,\n",
    "                'Retweet Count':retweet_count,\n",
    "                'Seguidores':followers_count})\n",
    "    #df.to_csv(\"Search Tweets.csv\")\n",
    "    df['Retificado'] = df['Retificado'].str.replace(\"[^a-zA-Z\\xC0-\\xFF]+\", \" \")  #tira vírgula e etc\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função acima retorna um dataframe com as mensagens e as informações. Cria ainda a coluna \"Retificado\", com o texto limpo de quaisquer caracteres que não sejam letras e palavras com 2 letras ou menos. elimina também mensagens retuitadas\n",
    "\n",
    "É possível captar mais informações. Basta consultar a referência no próprio Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uso da função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivos=tweetSearch(\"#positivos\", limit=10)  # retorna um Dataframe com até 10 tuites\n",
    "positivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O mesmo para capturar tuítes \"negativos\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativos=tweetSearch(\"#negativos\", limit=10)  # retorna um Dataframe com até 10 tuites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A escolha das hashtags que retornarão tuítes negativos e positivos é pessoal. Uma maneira comum é a busca por emoticons :)  e :(.  Acredito que a busca por #lixo ou #vergonha retornem mensagens negativas. Por outro lado #ferias #amor estejam presentes em mensagens positivas. Um modelo mais eficiente, depende de escolhas acertadas de conteúdo ou hashtag.\n",
    "\n",
    "Abaixo, Eliminamos mensagens replicadas e adicionamos a coluna de \"Tag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativos.drop_duplicates(subset='Message',keep='first', inplace=True)  #elimina duplicados mantém o primeiro \n",
    "positivos.drop_duplicates(subset='Message',keep='first', inplace=True)  \n",
    "\n",
    "negativos['Tag']=0\n",
    "positivos['Tag']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos querer capturar um número grande de mensagens, porém o twitter tem limitações. Uma dica é criar um banco SQL para salvar os tuites obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"tuites.db\") # ou use :memory: para botá-lo na memória RAM\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivos.to_sql(name='Positivos',con=conn,  if_exists='append')\n",
    "negativos.to_sql(name='Negativos',con=conn,  if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize as tabelas contidas na base de dados tuites.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Com o código acima, é possível obter dois dataframes (positivo e negativo) com milhares de mensagens. Recomendo no mínimo 20 mil de cada. As mensagens estão na coluna \"Retificado\"\n",
    "\n",
    "O passo seguinte é o \"stemming\", que consiste em reduzir as palavras ao seu radical. Por exemplo: amigo, amiga, amigável, fica reduzido a amig , economizando posições em nosso dicionário. \n",
    "Isso reduz o tamanho vocabulário sem comprometer a informação,assim podemos obter um modelo mais eficiente pela relação features / instâncias de treinamento.\n",
    "\n",
    "Já existe um pacote pronto que realiza isso em PORTUGUÊS, o snowball Stemmer.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro juntamos os dois dataframes em um único\n",
    "data=[positivos,negativos]\n",
    "dataset=pd.concat(data)\n",
    "dataset=dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "   # tokens = [x for x in tokens if not x in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Retificado']=dataset['Retificado'].apply(stem_sentences)\n",
    "\n",
    "#dê uma olhada como fica\n",
    "dataset['Retificado']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vetorizando\n",
    "\n",
    "Agora que já reduzimos as palavras, o passo seguinte é a vetorização. Nesse processo, todas as palavras são colocadas em ordem alfabética e nosso dataset será codificado em uma matriz:  TAMANHO DO DATASET X TAMANHO DO VOCABULÁRIO\n",
    "Lembrando que o vocabulário é formado pelas palavras de todas as mensagens \"stemmizadas\".\n",
    "\n",
    "Para a vetorização, vamos utilizar uma biblioteca do scikitlearn, que:\n",
    "- lista todas as palavras (previamente \"stemmizadas\") diferentes. Umas 18 mil\n",
    "- as coloca em ordem alfabética\n",
    "- vetoriza\n",
    "\n",
    "Assim, cada mensagem não importa o tamanho, se tornará um vetor de 18 mil palavras(+/-) onde 1 é a presença da palavra e zero, ausência.\n",
    "\n",
    "Todas as mensagens que no futuro serão analizadas pelo modelo, deverão ser vetorizadas com o mesmo vocabulário. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(dataset[\"Retificado\"])\n",
    "vocabulario=vectorizer.get_feature_names()  # usamos essa variável para obter o vocabulário de um vectorizer salvo via pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtendo o y\n",
    "y=dataset['Tag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o modelo\n",
    "\n",
    "Criaremos um com algoritmo Naive Bayes que é um algoritmo bem comum para análise de sentimento. Mas nada impede de utilizarmos outros que proporcionem um desempenho melhor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,shuffle=True)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train, y_train)\n",
    "y_pred  = nb.predict(X_test)\n",
    "print(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na própria base de dados, espere 70 - 80 % de precisão. \n",
    "Testando em outras bases de mensagens, o resultado fica por volta de 60%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando em uma nova mensagem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto=\"Esse twitter é cheio de haters. Que droga!\"\n",
    "corpus=[texto]\n",
    "\n",
    "#vetoriza\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulario)  # vamos vetorizar usando o vocabulário extraído da primeira vetorização\n",
    "J  = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "y_pred=nb.predict_proba(J)  #prevê probabilidade  %NEG  %POS [[0.8808889 0.1191111 ]]\n",
    "#y_pred=nb.predict(J)       # retorna 0 (negativo)  ou 1 (positivo)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
